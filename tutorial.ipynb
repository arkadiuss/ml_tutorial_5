{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN, AgglomerativeClustering, estimate_bandwidth\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór danych\n",
    "\n",
    "W zadaniu poddany analizie będzie zbiór MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./mnist_784\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data[:2000]\n",
    "y = mnist.target[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "X_scaled = standard_scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(30)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(X_pca)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(i) for i in range(10)]\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for c, label in zip(mcolors.TABLEAU_COLORS, ids):\n",
    "    plt.scatter(X_embedded[y == label, 0], X_embedded[y == label, 1], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=13)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "Spróbujmy zaprezentować obliczone centra przez k-means dla zredukowanego do przestrzeni dwuwymiarowej zbioru MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=10,\n",
    ").fit(X_embedded, y)\n",
    "\n",
    "labels = km.labels_\n",
    "cluster_centers = km.cluster_centers_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "for k, col in zip(range(n_clusters_), mcolors.TABLEAU_COLORS):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.scatter(X_embedded[my_members, 0], X_embedded[my_members, 1], c=col)\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker='s', c=col, s=500, edgecolors='black', linewidth='2')\n",
    "plt.title('Number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie wiemy które centrum odpowiada jakiej liczbie TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = estimate_bandwidth(X_embedded, quantile=0.05)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X_embedded)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "colors = dict(mcolors.TABLEAU_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.scatter(X_embedded[my_members, 0], X_embedded[my_members, 1], c=col, label=k)\n",
    "    plt.scatter(cluster_center[0], cluster_center[1], marker='s', c=col, s=500, edgecolors='black', linewidth='2')\n",
    "plt.title('Number of clusters: %d' % n_clusters_)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "Ten rodzaj clusteringu nie przewiduje obliczanych centrów, jednak wyróżnia punkty Noise (oznaczane na czarno) oraz punkty brzegowe (z czarnym obramowaniem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=3.7, min_samples=10).fit(X_embedded)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "\n",
    "colors = dict(mcolors.TABLEAU_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = 'black'\n",
    "    \n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X_embedded[class_member_mask & core_samples_mask]\n",
    "\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], c=col, label=k)\n",
    "    \n",
    "    xy = X_embedded[class_member_mask & ~core_samples_mask]\n",
    "    plt.scatter(xy[:, 0], xy[:, 1], c=col, s=30, edgecolors='black', linewidth='1')\n",
    "    \n",
    "plt.title('Number of clusters: %d' % n_clusters_)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "linkage = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "\n",
    "agg = AgglomerativeClustering(\n",
    "    n_clusters=10,linkage=linkage[1]\n",
    ").fit(X_embedded)\n",
    "\n",
    "labels = agg.labels_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "for k, col in zip(range(n_clusters_), mcolors.TABLEAU_COLORS):\n",
    "    my_members = labels == k\n",
    "    plt.scatter(X_embedded[my_members, 0], X_embedded[my_members, 1], c=col)\n",
    "plt.title('Number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możliwe jest wyświetlenie hierarchii, przy czym zmniejszymy zbiór dla większej szybkości wykonania obliczeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "test_size = 100\n",
    "X_small = mnist.data[:test_size]\n",
    "\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X_small)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs))\n",
    "        \n",
    "def plot_gmm(gmm, X):\n",
    "    ax = plt.gca()\n",
    "    labels = gmm.fit(X).predict(X)\n",
    "    print(labels)\n",
    "    if label:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1])\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    w_factor = 0.2 / gmm.weights_.max()\n",
    "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "        draw_ellipse(pos, covar, alpha=w * w_factor)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, covariance_type='full').fit(X_embedded)\n",
    "\n",
    "plot_gmm(gmm, X_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miary jakości\n",
    "\n",
    "Pomiary dokładności clustering'u bedą przeprowadzane na wcześniej przygotowanych danych.  \n",
    "Dla ułatwienie wszystkie przykłady korzystają z clustering'u KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import (silhouette_samples, silhouette_score, davies_bouldin_score, calinski_harabasz_score, \n",
    "                             davies_bouldin_score, pairwise_distances)\n",
    "from sklearn.metrics.cluster import contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=10, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja silhouette_samples oblicza wartość Silhouette dla każdej próbki\n",
    "sil_samples = silhouette_samples(X, labels)\n",
    "\n",
    "# Z kolei silhouete_score oblicza wartość średnią\n",
    "sil_score = silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "print(\"Silhouette samples:\")\n",
    "print(sil_samples)\n",
    "\n",
    "print(\"\\nSilhouette mean:\")\n",
    "print(sil_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caliński-Harabasz Index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski_score = calinski_harabasz_score(X, labels)\n",
    "\n",
    "print(\"Calinski-Harbasz index:\")\n",
    "print(calinski_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davies_score = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(\"Davies-Bouldin index:\")\n",
    "print(davies_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=10, random_state=1)\n",
    "y_pred = kmeans_model.fit_predict(X)\n",
    "\n",
    "contingency_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(model, X, y_true):\n",
    "    if hasattr(model, 'labels_'):\n",
    "        y_pred = model.labels_\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=10, random_state=1).fit(X)\n",
    "\n",
    "purity_score(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie metryk jakości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, X, y):\n",
    "    scores = []\n",
    "    \n",
    "    if type(model) is GaussianMixture:\n",
    "        labels = model.predict(X)\n",
    "    else:\n",
    "        labels = model.labels_\n",
    "    \n",
    "    scores.append(silhouette_score(X, labels, metric='euclidean'))\n",
    "    scores.append(calinski_harabasz_score(X, labels))\n",
    "    scores.append(davies_bouldin_score(X, labels))\n",
    "    scores.append(purity_score(model, X, y))\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X=X_train\n",
    "y=y_train\n",
    "\n",
    "index = ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'Purity']\n",
    "\n",
    "km_model = KMeans(n_clusters=10, random_state=1).fit(X)\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.05)\n",
    "ms_model = MeanShift(bandwidth=bandwidth).fit(X)\n",
    "db_model = DBSCAN(eps=3.7, min_samples=10).fit(X)\n",
    "agg_model = AgglomerativeClustering(n_clusters=10,linkage=\"complete\").fit(X)\n",
    "gmm_model = GaussianMixture(n_components=10, covariance_type='full').fit(X)\n",
    "\n",
    "\n",
    "d = {\"KMeans\": get_scores(km_model, X, y),\n",
    "     \"MeanShift\": get_scores(ms_model, X, y),\n",
    "     \"DBSCAN\": get_scores(db_model, X, y),\n",
    "     \"Agglomerative\": get_scores(agg_model, X , y), \n",
    "     \"GaussianMixture\": get_scores(gmm_model, X, y)} \n",
    "df = pd.DataFrame(d, index).transpose()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use 5 top clustering algorithm on FMIST or CIFAR 10 data set. Which algorithm is the best (on thebasis of clustering quality metrics).  \n",
    "BE CAREFUL. DO IT FOR THE BEST CLUSTERING PARAMETERS AND PROPERLY SELECTED NUMBER OF CLUSTERS.\n",
    "\n",
    "b) Do the same as in (a) but for the dataset transformed by PCA to 30-D space\n",
    "\n",
    "c) Does preclustering of data can help in achieving better performance of classification algorithms? How\n",
    "to do that? (top grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
